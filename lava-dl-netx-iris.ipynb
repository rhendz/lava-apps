{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lava.lib.dl.slayer as slayer\n",
    "\n",
    "import typing as ty\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from PIL import Image\n",
    "\n",
    "from lava.magma.core.run_configs import Loihi1SimCfg\n",
    "from lava.magma.core.run_conditions import RunSteps, RunContinuous\n",
    "from lava.proc.io.sink import RingBuffer as ReceiveProcess\n",
    "from lava.proc.io.source import RingBuffer as SendProcess\n",
    "from lava.proc import io\n",
    "\n",
    "from lava.lib.dl import netx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original slayer inference evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type X <class 'torch.Tensor'>\n",
      "Type y <class 'torch.Tensor'>\n",
      "Shape X torch.Size([4, 8])\n",
      "Shape y torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "# Slayer network model that was used to train IRIS model\n",
    "class Network(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        neuron_params = {\n",
    "                'threshold'     : 1.75,\n",
    "                'current_decay' : 0.25 , # this must be 1 to use batchnorm\n",
    "                'voltage_decay' : 0.03,\n",
    "                'tau_grad'      : 0.03,\n",
    "                'scale_grad'    : 3,\n",
    "                'requires_grad' : False,\n",
    "            }\n",
    "        \n",
    "        self.blocks = torch.nn.ModuleList([\n",
    "                slayer.block.cuba.Dense(neuron_params, 4, 24, weight_norm=True),\n",
    "                slayer.block.cuba.Dense(neuron_params, 24, 3, weight_norm=True),\n",
    "            ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Assume the tensor is in format NCT        \n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x\n",
    "    \n",
    "# IRIS dataset format used to train slayer based model\n",
    "class IrisDatasetSlayer(Dataset):\n",
    "    def __init__(self, data_file, label_file, transform=None, target_transform=None, time_steps=8):\n",
    "        features = pd.read_csv(data_file, header=None).values\n",
    "        labels = pd.read_csv(label_file, header=None).values\n",
    "        \n",
    "        self.X = torch.tensor(features).type(torch.FloatTensor)\n",
    "        self.X = self.X.reshape(150, 4, 1).repeat(1, 1, time_steps)\n",
    "        self.y = torch.tensor(labels).squeeze(1)\n",
    "        \n",
    "        self.samples = features.shape[0]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.samples\n",
    "    \n",
    "# IRIS dataset infos\n",
    "d = IrisDatasetSlayer('data/iris_data.csv', 'data/iris_label.csv')\n",
    "print('Type X', type(d[0][0]))\n",
    "print('Type y', type(d[0][1]))\n",
    "print('Shape X', d[0][0].shape)\n",
    "print('Shape y', d[0][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler, SequentialSampler\n",
    "\n",
    "# Setup dataset\n",
    "batch_size = 16\n",
    "test_split = 0.2\n",
    "shuffle_dataset = True\n",
    "random_seed = 42\n",
    "\n",
    "dataset = IrisDatasetSlayer('data/iris_data.csv', 'data/iris_label.csv')\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(test_split*dataset_size))\n",
    "if shuffle_dataset:\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "train_indices, test_indices = indices[split:], indices[:split]\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "# train_sampler = SequentialSampler(train_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "# Setup dataloader\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "test_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)\n",
    "\n",
    "# Setup network device\n",
    "# device = torch.device('cpu')\n",
    "device = torch.device('cuda') \n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[Epoch  0/1] Train  | Test  loss =    12.62833                        accuracy = 1.00000\n",
      "Correct samples (test set):  30 / 30\n",
      "\r",
      "[Epoch  0/1] Train  | Test  loss =    12.91690                        accuracy = 0.95333\n",
      "Correct samples (full set):  143 / 150\n"
     ]
    }
   ],
   "source": [
    "net = Network().to(device)\n",
    "net_pt = torch.load('Trained/network.pt')\n",
    "net.load_state_dict(net_pt)\n",
    "\n",
    "error = slayer.loss.SpikeMax(mode='softmax').to(device)\n",
    "stats = slayer.utils.LearningStats()\n",
    "assistant = slayer.utils.Assistant(net, error, optimizer, stats, classifier=slayer.classifier.Rate.predict)\n",
    "\n",
    "for epoch in range(1):\n",
    "    for i, (input, label) in enumerate(test_loader): # test set\n",
    "        output = assistant.test(input, label)\n",
    "    print(f'\\r[Epoch {epoch:2d}/{1}] {stats}')\n",
    "    print('Correct samples (test set): ',stats.testing.correct_samples,'/',stats.testing.num_samples)\n",
    "    \n",
    "for epoch in range(1):\n",
    "    for i, (input, label) in enumerate(train_loader): # train set\n",
    "        output = assistant.test(input, label)\n",
    "    print(f'\\r[Epoch {epoch:2d}/{1}] {stats}')\n",
    "    print('Correct samples (full set): ',stats.testing.correct_samples,'/',stats.testing.num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup network, dataloaders, resetters, and loggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24]\n",
      "[3]\n",
      "There are 2 layers in network:\n",
      "Dense : Process_8 , shape : (24,)\n",
      "Dense : Process_11, shape : (3,)\n"
     ]
    }
   ],
   "source": [
    "net = netx.hdf5.Network(net_config='Trained' + '/network.net')\n",
    "\n",
    "print(f'There are {len(net)} layers in network:')\n",
    "\n",
    "for l in net.layers:\n",
    "    print(f'{l.__class__.__name__:5s} : {l.name:10s}, shape : {l.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type X <class 'numpy.ndarray'>\n",
      "Type y <class 'numpy.int64'>\n",
      "Shape X (4, 8)\n",
      "Shape y ()\n",
      "[24]\n",
      "[3]\n"
     ]
    }
   ],
   "source": [
    "from lava.proc import io\n",
    "\n",
    "class IrisDatasetLava(Dataset):\n",
    "    def __init__(self, data_file, label_file, transform=None, target_transform=None, time_steps=8):\n",
    "        features = pd.read_csv(data_file, header=None).values\n",
    "        labels = pd.read_csv(label_file, header=None).values\n",
    "        \n",
    "        self.X = torch.tensor(features).type(torch.FloatTensor).unsqueeze(2)\n",
    "        self.X = self.X.reshape(150, 4, 1).repeat(1, 1, time_steps).numpy()\n",
    "        self.y = torch.tensor(labels).squeeze(1).numpy()\n",
    "\n",
    "        self.samples = features.shape[0]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.samples\n",
    "\n",
    "# Dataset infos\n",
    "d = IrisDatasetLava('data/iris_data.csv', 'data/iris_label.csv')\n",
    "print('Type X', type(d[0][0]))\n",
    "print('Type y', type(d[0][1]))\n",
    "print('Shape X', d[0][0].shape)\n",
    "print('Shape y', d[0][1].shape)\n",
    "\n",
    "num_samples = 150\n",
    "steps_per_sample = 8\n",
    "readout_offset = (steps_per_sample-1) + len(net.layers)\n",
    "num_steps = num_samples*steps_per_sample\n",
    "\n",
    "full_set = IrisDatasetLava('data/iris_data.csv', 'data/iris_label.csv')\n",
    "\n",
    "dataloader = io.dataloader.SpikeDataloader(\n",
    "    dataset=full_set,\n",
    "    interval=steps_per_sample,\n",
    ")\n",
    "\n",
    "net = netx.hdf5.Network(net_config='Trained' + '/network.net')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_logger = io.sink.RingBuffer(shape=(1,), buffer=num_steps)\n",
    "output_logger = io.sink.Read(\n",
    "    num_samples,\n",
    "    interval=steps_per_sample,\n",
    "    offset=readout_offset\n",
    ")\n",
    "\n",
    "# reset after every sample has been run\n",
    "for i, l in enumerate(net.layers):\n",
    "    u_resetter = io.reset.Reset(interval=steps_per_sample, offset=i)\n",
    "    v_resetter = io.reset.Reset(interval=steps_per_sample, offset=i)\n",
    "    u_resetter.connect_var(l.neuron.u)\n",
    "    v_resetter.connect_var(l.neuron.v)\n",
    "\n",
    "dataloader.ground_truth.connect(gt_logger.a_in)\n",
    "dataloader.s_out.connect(net.in_layer.inp)\n",
    "\n",
    "output_logger = io.sink.RingBuffer(shape=net.out_layer.shape, buffer=num_steps)\n",
    "net.out_layer.out.connect(output_logger.a_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure/run network for inference on Loihi1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRunConfig(Loihi1SimCfg):\n",
    "    def select(self, proc, proc_models):\n",
    "        # customize run config to always use float model for io.sink.RingBuffer\n",
    "        if isinstance(proc, io.sink.RingBuffer):\n",
    "            return io.sink.PyReceiveModelFloat\n",
    "        else:\n",
    "            return super().select(proc, proc_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = CustomRunConfig(select_tag='fixed_pt')\n",
    "net.run(condition=RunSteps(num_steps=num_steps), run_cfg=run_config)        \n",
    "output = output_logger.data.get()\n",
    "gts = gt_logger.data.get().flatten()[::steps_per_sample]\n",
    "# net.pause()\n",
    "net.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output accuracy and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.3333333333333333\n",
      "[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "gts_arr = gts\n",
    "output_arr = output.reshape(3,150,8)\n",
    "output_arr = np.swapaxes(output_arr, 0, 1)\n",
    "\n",
    "classifier = slayer.classifier.Rate()\n",
    "prediction = classifier.predict(torch.from_numpy(output_arr)).numpy()\n",
    "print('Accuracy', np.sum(gts_arr == prediction)/len(prediction))\n",
    "print(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
